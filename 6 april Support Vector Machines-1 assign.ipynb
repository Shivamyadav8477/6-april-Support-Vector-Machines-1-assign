{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0219589a-d901-471f-83d5-46dbea345636",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is the mathematical formula for a linear SVM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af19d75-02b6-43c7-be18-ada1266af9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "The mathematical formula for a linear Support Vector Machine (SVM) can be expressed as follows:\n",
    "\n",
    "Given a dataset with N data points, each represented as a feature vector xᵢ in a D-dimensional space, and corresponding binary class labels yᵢ ∈ {-1, +1}, a linear SVM aims to find a hyperplane that best separates the data into two classes while maximizing the margin between the two classes.\n",
    "\n",
    "1. **Objective Function (Primal Formulation):**\n",
    "\n",
    "   The objective of a linear SVM can be formulated as an optimization problem:\n",
    "\n",
    "   **Minimize:**\n",
    "   ![SVM Primal Objective](https://latex.codecogs.com/png.latex?%5Cfrac%7B1%7D%7B2%7D%20%5Cleft%5C%7C%20w%20%5Cright%5C%7C%5E2%20+%20C%20%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Cxi_i)\n",
    "\n",
    "   where:\n",
    "   - **w** is the weight vector of the hyperplane.\n",
    "   - **C** is the regularization parameter (a hyperparameter that controls the trade-off between maximizing the margin and minimizing classification error).\n",
    "   - **ξᵢ** represents the slack variable associated with the i-th data point, allowing for some misclassification.\n",
    "\n",
    "2. **Constraints:**\n",
    "\n",
    "   The linear SVM is subject to the following constraints:\n",
    "\n",
    "   ![SVM Constraints](https://latex.codecogs.com/png.latex?y_i%20%28w%20%5Ccdot%20x_i%20%20&plus;%20%20b%29%20%5Cgeq%201%20-%20%5Cxi_i%20%5Cquad%20%5Ctext%7Bfor%20%7D%20i%20%3D%201%2C%202%2C%20%5Cldots%2C%20N)\n",
    "\n",
    "   - **yᵢ** is the class label of the i-th data point (+1 or -1).\n",
    "   - **(w ⋅ xᵢ + b)** is the decision boundary (hyperplane) that should correctly classify data points.\n",
    "   - The inequality indicates that data points should lie on the correct side of the hyperplane with a margin of at least 1, or the slack variable **ξᵢ** accounts for any misclassification.\n",
    "\n",
    "3. **Objective Function (Dual Formulation):**\n",
    "\n",
    "   The linear SVM problem is often transformed into its dual form, involving the solution for Lagrange multipliers (αᵢ):\n",
    "\n",
    "   **Maximize:**\n",
    "   ![SVM Dual Objective](https://latex.codecogs.com/png.latex?%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Calpha_i%20-%20%5Cfrac%7B1%7D%7B2%7D%20%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Csum_%7Bj%3D1%7D%5E%7BN%7D%20%5Calpha_i%20%5Calpha_j%20y_i%20y_j%20%28x_i%20%5Ccdot%20x_j%29)\n",
    "\n",
    "   subject to:\n",
    "   ![SVM Dual Constraints](https://latex.codecogs.com/png.latex?0%20%5Cleq%20%5Calpha_i%20%5Cleq%20C%20%5Cquad%20%5Ctext%7Bfor%20%7D%20i%20%3D%201%2C%202%2C%20%5Cldots%2C%20N)\n",
    "   ![SVM Dual Summation Constraint](https://latex.codecogs.com/png.latex?%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Calpha_i%20y_i%20%3D%200)\n",
    "\n",
    "   - **αᵢ** are the Lagrange multipliers.\n",
    "   - The objective is to maximize the sum of Lagrange multipliers while minimizing a term that depends on the inner product of data points.\n",
    "   - The constraints ensure that αᵢ values are non-negative and bounded by **C**, and that the sum of (αᵢ * yᵢ) is zero.\n",
    "\n",
    "4. **Decision Boundary:**\n",
    "\n",
    "   The decision boundary can be expressed as:\n",
    "\n",
    "   ![SVM Decision Boundary](https://latex.codecogs.com/png.latex?w%20%3D%20%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12facf57-c090-41ac-8169-5a45a76437e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. What is the objective function of a linear SVM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635e89db-8a49-4e1b-bdbc-2a4d7a79647a",
   "metadata": {},
   "outputs": [],
   "source": [
    "The objective function of a linear Support Vector Machine (SVM) aims to find the optimal hyperplane that best separates a given dataset into two classes while maximizing the margin between the classes. The objective function for a linear SVM can be formulated as follows:\n",
    "\n",
    "**Objective Function (Primal Formulation):**\n",
    "\n",
    "Minimize:\n",
    "\n",
    "![SVM Primal Objective](https://latex.codecogs.com/png.latex?%5Cfrac%7B1%7D%7B2%7D%20%5Cleft%5C%7C%20w%20%5Cright%5C%7C%5E2%20+%20C%20%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Cxi_i)\n",
    "\n",
    "where:\n",
    "- **w** is the weight vector of the hyperplane.\n",
    "- **C** is the regularization parameter (a hyperparameter that controls the trade-off between maximizing the margin and minimizing classification error).\n",
    "- **ξᵢ** represents the slack variable associated with the i-th data point, allowing for some misclassification.\n",
    "\n",
    "The components of the objective function are as follows:\n",
    "- The first term, ![SVM Margin Term](https://latex.codecogs.com/png.latex?%5Cfrac%7B1%7D%7B2%7D%20%5Cleft%5C%7C%20w%20%5Cright%5C%7C%5E2), represents the margin that we want to maximize. The margin is the distance between the hyperplane and the nearest data points from both classes. By minimizing the squared norm of **w**, we maximize the margin.\n",
    "- The second term, ![SVM Slack Term](https://latex.codecogs.com/png.latex?C%20%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Cxi_i), introduces slack variables **ξᵢ** to account for misclassified data points. The parameter **C** controls the trade-off between maximizing the margin and allowing for some misclassification. A smaller **C** encourages a larger margin but allows more misclassification, while a larger **C** penalizes misclassification more heavily and may result in a smaller margin.\n",
    "\n",
    "The optimization problem is subject to constraints that ensure data points are correctly classified or lie within a certain margin of the hyperplane. The primal formulation of the SVM seeks to minimize the sum of the squared norm of the weight vector while respecting these constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa8936b-7007-460f-a6dd-50ab175a69cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. What is the kernel trick in SVM?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adbc69b-c078-454b-99e1-971a7dc93883",
   "metadata": {},
   "outputs": [],
   "source": [
    "The kernel trick is a fundamental concept in Support Vector Machines (SVMs) that allows these models to handle non-linearly separable data in a higher-dimensional space without explicitly computing the transformation. It is a powerful technique that enables SVMs to capture complex patterns and decision boundaries.\n",
    "\n",
    "Here's how the kernel trick works:\n",
    "\n",
    "1. **Linear Separation in a Higher-Dimensional Space:** SVMs aim to find a hyperplane that best separates data points into different classes. In cases where the data is not linearly separable in the original feature space, the kernel trick allows SVMs to implicitly map the data into a higher-dimensional space where it might become linearly separable.\n",
    "\n",
    "2. **Kernel Functions:** Instead of explicitly transforming the data, SVMs use kernel functions to compute the dot product (inner product) between the transformed feature vectors in the higher-dimensional space without ever computing the transformation explicitly. These kernel functions measure the similarity or distance between data points in the higher-dimensional space.\n",
    "\n",
    "3. **Common Kernel Functions:** There are several common kernel functions used in SVMs, including:\n",
    "   - **Linear Kernel (K(x, y) = xᵀy):** This is the standard linear SVM that works in the original feature space.\n",
    "   - **Polynomial Kernel (K(x, y) = (xᵀy + c)ᵈ):** It maps data into a higher-dimensional polynomial space.\n",
    "   - **Radial Basis Function (RBF) Kernel (K(x, y) = exp(-γ‖x - y‖²)):** It maps data into an infinite-dimensional space and is useful for capturing complex non-linear relationships.\n",
    "   - **Sigmoid Kernel (K(x, y) = tanh(αxᵀy + c)):** It maps data into a space resembling a neural network sigmoid activation.\n",
    "\n",
    "4. **Benefits:** The kernel trick provides several advantages:\n",
    "   - It allows SVMs to capture complex decision boundaries and handle non-linear data effectively.\n",
    "   - It avoids the computational burden of explicitly transforming data into a higher-dimensional space.\n",
    "   - It can work in spaces of much higher dimensionality than the original feature space without the need to compute the explicit transformation.\n",
    "\n",
    "5. **Choosing the Right Kernel:** Selecting the appropriate kernel function and tuning its hyperparameters is crucial for the success of an SVM model. The choice of kernel depends on the nature of the data and the specific problem.\n",
    "\n",
    "In summary, the kernel trick is a key feature of SVMs that enables them to handle non-linearly separable data by implicitly mapping it into higher-dimensional spaces using kernel functions. This allows SVMs to find complex decision boundaries and perform well in a wide range of classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9a8ba9-8086-4535-9e97-f74d6db4972a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What is the role of support vectors in SVM Explain with example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd894eb-f2f3-4a59-8521-ee88a88dc2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Support vectors play a crucial role in Support Vector Machines (SVMs) and are the data points that are closest to the decision boundary (hyperplane). These support vectors directly influence the position and orientation of the decision boundary. The key role of support vectors in SVM can be explained with an example:\n",
    "\n",
    "**Example: Binary Classification**\n",
    "\n",
    "Let's consider a binary classification problem where we want to separate two classes, \"Positive\" and \"Negative,\" using an SVM. In a two-dimensional feature space, the decision boundary is a hyperplane. The support vectors are the data points that are closest to this hyperplane. These points are the ones that have the smallest margin or are \"supporting\" the hyperplane.\n",
    "\n",
    "Here's a step-by-step explanation:\n",
    "\n",
    "1. **Data Points:** We have a dataset with two classes, \"Positive\" and \"Negative,\" represented in the feature space.\n",
    "\n",
    "2. **Finding the Hyperplane:** The SVM algorithm's goal is to find the hyperplane that maximizes the margin (distance) between the two classes. The margin is defined as the perpendicular distance from the hyperplane to the nearest data point.\n",
    "\n",
    "3. **Support Vectors:** The support vectors are the data points that lie on the margin boundaries or are closest to the decision boundary. These points are crucial because they determine the position and orientation of the hyperplane.\n",
    "\n",
    "4. **Margin:** The margin is defined by the distances between the support vectors and the decision boundary (hyperplane). The SVM aims to maximize this margin.\n",
    "\n",
    "5. **Margin Optimization:** The SVM optimization process involves finding the hyperplane that not only separates the classes but also maximizes the margin. The support vectors are instrumental in this optimization process. The margin is effectively determined by the distances between the support vectors and the hyperplane.\n",
    "\n",
    "6. **Classification:** Once the SVM has identified the hyperplane, it can classify new data points based on which side of the hyperplane they fall. If a new data point is on the same side as most of the support vectors, it is classified into the corresponding class.\n",
    "\n",
    "**Role of Support Vectors:**\n",
    "- Support vectors influence the position and orientation of the decision boundary, as they are the closest points to the boundary.\n",
    "- They determine the margin, which reflects the robustness of the classifier. A larger margin indicates a more robust classifier.\n",
    "\n",
    "In summary, support vectors are the critical data points that have the smallest margin to the decision boundary in an SVM. They play a pivotal role in determining the optimal hyperplane and, consequently, the classifier's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9989af0b-964d-46c6-a467-32fa631c9a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. SVM Implementation through Iris dataset.\n",
    "\n",
    "Bonus task: Implement a linear SVM classifier from scratch using Python and compare its\n",
    "performance with the scikit-learn implementation.\n",
    "~ Load the iris dataset from the scikit-learn library and split it into a training set and a testing setl\n",
    "~ Train a linear SVM classifier on the training set and predict the labels for the testing setl\n",
    "~ Compute the accuracy of the model on the testing setl\n",
    "~ Plot the decision boundaries of the trained model using two of the featuresl\n",
    "~ Try different values of the regularisation parameter C and see how it affects the performance of\n",
    "the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdd467f-b5d4-4137-8f29-c6302d54d5ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc942eb0-d33b-4b44-9a16-c018e6d8e871",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c97b63-7ccf-4028-8b6d-be1454faa78e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
